---
title: TiKV 源码阅读(一) -- KV Service
excerpt: 介绍 TiKV 中 KV Service 的实现和调用。
layout: post
categories: TiKV
---

{% include toc %}

## grpc
TiKV 使用 grpc 作为通讯框架，只需要定义、实现 service 并注册到 grpc 即可实现网络通讯。grpc 内部会有多个 eventloop 线程，当有完整的请求到达时，就会在 eventloop 线程中调用注册的服务
接口，所以接口不能太重，一般会把请求转发到其他的线程去执行。

grpc-rs 是基于 future 封装的，是为了从回调地狱中解脱出来，使得代码的编写更类似于同步，但是 TiKV 中还是有不少地方是用 callback 串联起来的，比如写相关的请求，毕竟 callback 的模式更加灵活。

TiKV 提供了 2 套接口：
1. **raw kv**: 提供最基础 kv 接口。
2. **transactional kv**: 提供基于 percolator 的分布式事务接口。

这次只关注 transactional kv 的调用，事务的逻辑在之后再分析。

## Storage
KV Service 调用的基本都是 `Storage` 的接口，`Storage` 调用的都是 `Engine` 的接口:
* `Engine` 提供了最根本的支持 column family 的异步 kv 读、遍历和写的功能，当然还要保证强一致性。
* `Storage` 在 `Engine` 的基础上实现了分布式事务的支持。

`Storage` 将读操作转发到 `ReadPool` 中执行，而写操作会由 `Scheduler` 管理，目的是提供 `percolator` 需要的行级事务的支持。

## ReadPool
`ReadPool` 是支持优先级的线程池，内部由 3 个 `FuturePool` 组成，可以通过配置 `[readpool.storage]` 设置大小等参数。
`FuturePool` 是对 `tokio_threadpool` 的封装，增加了 `metrics`。 client 请求中会携带优先级，用于选择相应 pool 来执行。

读操作需要调用 `Engine::async_snapshot()` 来获取 snapshot，所以需要传递 `Engine` 到 `ReadPool` 中，一种方式是 Clone 通过闭包传递过去，但是
测试发现对性能有些影响，所以现在是通过 `tokio_threadpool::after_start()` 在每个线程启动后设置 thread_local 的 `Engine` 引用。

## Scheduler
percolator 需要行级事务的支持，即对单行多个 column family 的操作是原子的，而 `Engine` 没有提供事务的支持，好处是对 `Engine` 的约束小，底层实现可以有更多的选择。
TiKV 是通过 `storage/txn/scheduler` 对写操作进行调度，实现了类似行级事务的功能。

### Latches
在 `Scheduler` 内部有个 `Latches`，用于请求的冲突解决，基本思想是：只要有可能发生冲突的请求就会排队，上一个执行完成后才能执行下一个：

![images](/assets/images/tikv/latch.svg)

`Latches` 包含多个 slot，每个 slot 是一个队列，可以通过 `scheduler-concurrency` 配置 slot 的个数，默认为 2048000 个。
请求只有获取到它所需的所有 slot 才能执行，slot 是根据 key hash 得到，排在队首的即当前 slot 的拥有者，当请求执行完成后就会释放 slot，从队列中移除并尝试唤醒之后的请求。

这里类似于加锁，也要防止发生死锁：
* 对请求要加锁的 slot 排序，保证加锁顺序。
* 只有获取到之前的 slot 才能对下一个 slot 加锁。

已上图为例，Cmd 1 获取到了所有 slots，可以执行，而 Cmd 2 在等待 slot 4 的释放，Cmd 3 只有获取到 slot 1 后才能尝试加锁 slot 2。当 Cmd 1 执行完成释放掉 slots 后，
Cmd 2/3 才可以同时执行。

当发现在 `Latches` 这里等待时间太久(可查看 `tikv_scheduler_latch_wait_duration_seconds`)，即冲突比较严重时，可以尝试调大配置 `scheduler-concurrency`。

### Execute
和读不同的是，写请求不是立即转发到其他线程。`Scheduler` 是线程安全的，从 `Scheduler` 获取 latch 是在 grpc 线程中执行的，
只有请求可以执行时才会转发到 `SchedPool` 中，`SchedPool` 也是 `FuturePool`，可通过配置 `scheduler-worker-pool-size` 设置大小。这对性能是否有影响还需要测试。

在之前的版本中，`Scheduler` 是单线程的，有请求到达时会通过 channel 发送给 `Scheduler`，可以执行时再转发给 worker pool，当请求执行完成后还需要通知 `Scheduler` 释放 latch，
现在在各个线程中直接调用即可。
